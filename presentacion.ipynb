{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09af8cf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "<br><br><br><br><br><br>\n",
    "Materia: Aprendizaje Automatico <br>\n",
    "Docente: Esteban Roitberg <br>\n",
    "Alumno: Gonzalo Jara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2026742",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Qué es PCA?\n",
    "<br><br>\n",
    "Es una técnica de reducción de dimensionalidad. <br>\n",
    "Nos permite tomar una matriz de datos $X: n \\times p$ con un $p$ muy grande y expresarla en un subespacio de \"componentes principales\" <br>\n",
    "\n",
    "Por ejemplo: <br>\n",
    "$X: n \\times 50 \\rightarrow \\text{magia PCA} \\rightarrow U: n \\times 2$ <br>\n",
    "\n",
    "Esas dos columnas son los 2 subespacios mas importantes de la matriz <br>\n",
    "No es lo mismo que tomar las columnas/variables mas importantes <br>\n",
    "\n",
    "¿Qué es esa \"magia PCA\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ea4ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Cuál es la idea de PCA?\n",
    "\n",
    "<div class=\"slide-content\">\n",
    "  <div class=\"img-container\">\n",
    "    <img src=\"img/proyectados.png\" height=\"411\" width=\"424\"/>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"text-container\">\n",
    "    <p>Se busca un subespacio <strong>u</strong> que <br>\n",
    "    maximize la varianza de los datos proyectados <br> <br>\n",
    "    ¿Por qué se proyecta? <br>\n",
    "    ¿Por qué la varianza? </p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    ".slide-content {\n",
    "    display: flex;\n",
    "    flex-direction: row;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "    gap: 40px;\n",
    "}\n",
    "\n",
    ".img-container {\n",
    "    flex: 1;\n",
    "    text-align: center;\n",
    "}\n",
    "\n",
    ".text-container {\n",
    "    flex: 1;\n",
    "    font-size: 1.2em;\n",
    "    line-height: 1.5em;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9de08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"slide-content\">\n",
    "  <div class=\"img-container\">\n",
    "    <img src=\"img/ejemplo2.png\" height=\"593\" width=\"589\"/>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"text-container\">\n",
    "    <p>\n",
    "    Dos nubes claramente diferenciables<br><br>\n",
    "    x1 puede separar algo info<br>\n",
    "    x2 no logra ver la diferencia<br><br>\n",
    "    Buscar subespacios donde las proyecciones maximizen la varianza</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    ".slide-content {\n",
    "    display: flex;\n",
    "    flex-direction: row;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "    gap: 40px;\n",
    "}\n",
    "\n",
    ".img-container {\n",
    "    flex: 1;\n",
    "    text-align: center;\n",
    "}\n",
    "\n",
    ".text-container {\n",
    "    flex: 1;\n",
    "    font-size: 1.2em;\n",
    "    line-height: 1.5em;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac7a8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"slide-content\">\n",
    "  <div class=\"img-container\">\n",
    "    <img src=\"img/ejemplo2_centrado.png\" height=\"593\" width=\"589\"/>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"text-container\">\n",
    "    <p>\n",
    "    Datos centrados ¿No escalados? <br>\n",
    "    Cada flecha es un subespacio/Componente <br>\n",
    "    ¿Que significa el largo de la flecha? <br>\n",
    "    <br> ¿Por qué sigo mencionando la varianza? <br>\n",
    "    </p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    ".slide-content {\n",
    "    display: flex;\n",
    "    flex-direction: row;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "    gap: 40px;\n",
    "}\n",
    "\n",
    ".img-container {\n",
    "    flex: 1;\n",
    "    text-align: center;\n",
    "}\n",
    "\n",
    ".text-container {\n",
    "    flex: 1;\n",
    "    font-size: 1.2em;\n",
    "    line-height: 1.5em;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb42b71",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"slide-content\">\n",
    "  <div class=\"img-container\">\n",
    "    <img src=\"img/ejemplo2_proyectado.png\" height=\"593\" width=\"589\"/>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"text-container\">\n",
    "    <p>\n",
    "      PC1 captura mejor la varianza <br>\n",
    "      Puedo usar solo PC1 para explicar mis datos <br>\n",
    "      <br>Extrapolar a matrices con muchas variables <br>\n",
    "      <br>Eso es lo que hace PCA <br>\n",
    "    </p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    ".slide-content {\n",
    "    display: flex;\n",
    "    flex-direction: row;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "    gap: 40px;\n",
    "}\n",
    "\n",
    ".img-container {\n",
    "    flex: 1;\n",
    "    text-align: center;\n",
    "}\n",
    "\n",
    ".text-container {\n",
    "    flex: 1;\n",
    "    font-size: 1.2em;\n",
    "    line-height: 1.5em;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03665121",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Cómo funciona por dentro?\n",
    "Matriz de datos $C = X - \\bar{X}$ de forma $C: N \\times p$ <br>\n",
    "Matriz de subespacios $V = [v_1 \\, v_2 \\, ... \\, v_p]$ de forma $V: p \\times p$ <br>\n",
    "Los datos proyectados se les llama $Z_{score} = C \\cdot V$ de forma $Z_{score}: N \\times p$ <br>\n",
    "\n",
    "Se busca hallar vectores $v_p$ que maximizen la varianza de los datos proyectados <br>\n",
    "<!-- Recordar que se busca atrapar la mayor cantidad de varianza posible -->\n",
    "$$\n",
    "Var(Z_1) = \\frac{1}{N - 1} \\sum_{n=1}^{N} \\{ C_n \\cdot v_1 \\}^2 = v_1^T S v_1\n",
    "$$ <br>\n",
    "\n",
    "Con una restriccion: Quiero que el vector $v_1$ sea unitario\n",
    "$$\n",
    "v_1^T \\cdot v_1 = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be36ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usamos multiplicadores de Lagrange\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(v_1, \\lambda) = v_1^T S v_1 - \\lambda (v_1^T \\cdot v_1 - 1)\n",
    "$$ <br>\n",
    "Derivamos respecto a $v_1$ y hallamos los máximos\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial v_1} = 2 S v_1 - 2\\lambda v_1 = 0\n",
    "$$ <br>\n",
    "\n",
    "$$\n",
    "S v_1 = \\lambda v_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a8090",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Autovectores!! <br>\n",
    "Entonces, para hallar cada subespacio, se deben calcular los autovectores de $S = \\frac{1}{N - 1} C^T C$ <br>\n",
    "Pasado a limpio, nos queda la descomposición $S = V \\; D \\; V^T$\n",
    "<div class=\"slide-content\">\n",
    "  <div class=\"img-container\">\n",
    "    <img src=\"img/ejemplo2_centrado.png\" height=\"593\" width=\"589\"/>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"text-container\">\n",
    "    <p>\n",
    "        Entonces: <br>\n",
    "        Al calcular los autovectores se está resolviendo un problema de PCA <br>\n",
    "    </p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    ".slide-content {\n",
    "    display: flex;\n",
    "    flex-direction: row;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "    gap: 40px;\n",
    "}\n",
    "\n",
    ".img-container {\n",
    "    flex: 1;\n",
    "    text-align: center;\n",
    "}\n",
    "\n",
    ".text-container {\n",
    "    flex: 1;\n",
    "    font-size: 1.2em;\n",
    "    line-height: 1.5em;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51db2e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Relación entre PCA y SVD?\n",
    "Solo hay que definir $C = U \\; D \\; V^T$ <br>\n",
    "\n",
    "Recuerden que, para resolver PCA es necesario calcular la matriz de covarianza $S = \\frac{1}{N - 1} \\; C^T C$ <br>\n",
    "Aplicando SVD nos quedaria:\n",
    "$$\n",
    "S = \\frac{1}{N - 1} \\; (U \\; D \\; V^T)^T (U \\; D \\; V^T)\n",
    "$$\n",
    "$$\n",
    "S = \\frac{1}{N - 1} (V \\; D \\; U^T) (U \\; D \\; V^T)\n",
    "$$\n",
    "$$\n",
    "S = \\frac{1}{N - 1} \\;\\; (V \\; D^2 \\; V^T)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b6bfa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA y SVD\n",
    "Esto nos indica que, al aplicar SVD, se obtienen las matrices que necesita PCA: <br>\n",
    "* $U$: son los datos expresados en los subespacios generados por los autovectores.\n",
    "* $V^T$: es la matriz de autovectores.\n",
    "* $\\frac{D^2}{N - 1}$: es una matriz diagonal compuesta por la varianza de las proyecciones en cada autovector (Además está ordenada).\n",
    "* $U \\cdot D$: es equivalente a las proyecciones de $C$ sobre los autovectores/subespacios. (tambien es lo mismo que $C \\cdot V^T$). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13adf2ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo con datos reales\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9661ac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusiones\n",
    "\n",
    "PCA es una técnica muy útil para reducir la dimensionalidad de un conjunto de datos y también para encontrar posibles relaciones entre las variables. <br>\n",
    "\n",
    "Aplicaciones comunes:\n",
    "* Reducción de dimensionalidad.\n",
    "* Compresión de datos. \n",
    "* Extracción de features.\n",
    "* Visualizaciones.\n",
    "* Completar matrices.\n",
    "* Análisis exploratorio\n",
    "* Aprendizaje no supervisado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aprendizaje-automatico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
